---
layout: post
title: "Recurrent Neural Network(RNN) - Many to Many"
date: 2018-12-21 
desc: "Recurrent Neural Network(RNN) - Many to Many"
keywords: "RNN, LSTM, Many to Many"
categories: [Deep learning]
tags: [RNN, LSTM, Many to Many]
icon: icon-html
---

이번 포스팅에서는 저번 RNN 포스팅에 이어서 RNN의 구조 중 하나인 Many to Many의 구조에 대해서 자세히 알아보겠습니다. 그냥 구조만 설명된 글은 저번 포스팅에서 했으니, 이번에는 직접 데이터를 이용해 many to many 모델을 tensorflow라는 라이브러리로 구현해보면서 설명해보겠습니다.  

---

## 1. 모델 구현 준비  
##### 모듈과 스케일링 함수 및 parameter 정의
```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas
from pandas import DataFrame

tf.set_random_seed(777)  # reproducibility 

def MinMaxScaler(data):
    numerator = data - np.min(data, 0)
    denominator = np.max(data, 0) - np.min(data, 0)
    return numerator / (denominator + 1e-7)

# train Parameters
seq_length = 25
data_dim = 385
hidden_dim = 20
output_dim = 385
predict_day = 1
learning_rate = 0.01
iterations = 500
LSTM_stack = 1
```
- 시드값을 정해주지 않으면 매번 학습시킬때마다 초기값이 다르게 설정되어 결과가 다르게 나올 수 있습니다. 그리고 제가 사용하고자 하는 시계열데이터는 변수마다의 스케일이 매우 다르기도 하고, LSTM Cell을 통해 나온 값에 대해 학습이 수렴되지 않는 문제가 생기기도 하기 때문에 minmax 스케일링 작업을 거쳤습니다.  
- 제가 사용한 데이터는 변수가 약 350개, record가 약 3000여개의 데이터로, pandas의 데이터프레임 형식을 이용해서 저장을 시켰습니다.  
![](https://i.imgur.com/vj9BZ7l.png?1)  

##### 인풋 데이터와 아웃풋 데이터 정의
그런데 저는 여기서 이제 몇 개의 레코드씩 학습을 시킬지, 3000개의 정보를 한번에 학습시키는 것이 아니라 일정 길이의 인풋씩 잘라서 학습을 시킬지 정했습니다. 

```python
dataX = []
dataY = []
dataY_temp = []
for i in range(0, len(dataset) - seq_length-predict_day+1):
    _x = dataset[i:i + seq_length]
    _y = dataset[i + predict_day:i + seq_length+predict_day]
    #print(i + seq_length+predict_day)
    dataX.append(_x)
    dataY.append(_y)
x = np.array(dataX)
y = np.array(dataY)
```
위의 코드를 보면 알 수 있듯이, 저는 seq_length 즉 25일씩 학습을 시켰습니다. 아래 사진처럼 day1~25를 넣어서 day2~26을 아웃풋으로 하여 결과를 비교했고, 1~25, 2~26, 3~27 이렇게 순서대로 25일치씩을 학습해서 끝단의 26, 27, 28일의 아웃풋값을 해당 일자를 대표하는 값으로 할당했습니다. 이렇게 하면 아래그림에서 마지막 아웃풋 값인 26일 벡터는 1~25일의 데이터를 반영한 하나의 벡터가 되는 것입니다.(후에 연속적인 날의 흐름을 반영한 최종 벡터들을 이용해 plot을 해보기 위함)  

![](https://i.imgur.com/u3EB8BX.png?1)
 
---

## 2. 텐서플로우를 통한 RNN 구조 구성  
##### LSTM 네트워크 구성  
```python
# input place holders
X = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='intput_X')
Y = tf.placeholder(tf.float32, [None, seq_length, data_dim], name='intput_Y')

# build a LSTM network
def lstm_cell():
    cell = tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True) 
    return cell

multi_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(LSTM_stack)], state_is_tuple=True)


outputs_rnn, _states=tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)
```
위에서 만든 인풋 데이터를 텐서플로우라는 프레임워크에 흘려보내기 위해, 인풋의 사이즈를 받아들일 수 있는 placeholder를 구성했습니다. 그리고 1번의 마지막 그림에서 보이는 것처럼 hidden layer의 cell들을 25개의 인풋을 넣을 수 있는 LSTM cell로 구성했습니다.
  

##### fully-connected layer  
CNN 모델에서 끝단에 fully-connected layer를 쓰는것처럼, many to many의 결과 벡터에 softmax를 사용하는 방식을 추가했습니다. 여기서도 코드를 보면 인풋 형태를 앞 과정과 맞춰주기 위해 shape를 바꿔주어야 합니다. 텐서플로우에서는 정해진 틀에다가 데이터를 흘린다는 개념이기 때문에 항상 정의를 잘 해주어야 합니다.
```python
X_for_fc = tf.reshape(outputs_rnn, [-1, hidden_dim])
Y_pred_temp = tf.contrib.layers.fully_connected(X_for_fc, output_dim, activation_fn=None)  
# reshape out for sequence_loss
Y_pred = tf.reshape(Y_pred_temp, [-1, seq_length,output_dim])
```

##### loss function 및 optimizer 정의
학습을 시키고자 하는 loss function을 MSE값을 이용했고, optimzer 는 adam을 이용했습니다.
```python
mean_loss = tf.reduce_mean(tf.square(Y_pred - Y), name='losses_mean')  # mean of the squares
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)
```
---

## 3. 본격적인 학습  
데이터를 8:2 정도로 분할해서 80%로 모델을 학습시키고, 그 모델로 20%를 예측하면서 그 loss값들을 저장해보았습니다. 25일씩 전체를 다 학습한 뒤에 그 과정을 500번 반복하면서 loss를 최소화시켰습니다.
```python 
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)


losslist1 = [];
losslist2 = [];

# Training step       
for i in range(iterations):

    _, step_loss = sess.run([optimizer, mean_loss], 
                                    feed_dict={X: train_X, 
                                               Y: train_Y_label})
   
    
    step_loss2 = sess.run([mean_loss], 
                                    feed_dict={X: test_X, 
                                               Y: test_Y_label})
    losslist1 = np.append(losslist1, step_loss)
    losslist2=np.append(losslist2, step_loss2)
```    
![](https://i.imgur.com/7cYHIw8.png)  
##### parameter 조정  
위 그림에서 보면, 아무리 반복해도 loss가 일정수준 밑으로 떨어지지 않는 것을 볼 수 있습니다. 그러면 underfitting이 되있을 수도 있단 뜻인데, 데이터의 정보를 적절하게 반영하지 못한다는 것입니다. 그래서 해결방안으로는 cell 내부의 dimension을 확장시키거나, LSTM 층을 더 두껍게 쌓거나 학습률을 바꿔보는 등 조정을 해볼 수 있습니다. 여러 실험을 통해 전 이전보다 훨씬 좋은 결과를 얻어냈습니다.  
![](https://i.imgur.com/RkMyijm.png)

---

## 4. 결론  
기본적인 구조의 모델 자체를 구성하는 것은 어렵지 않으나, 해당 데이터에서 자신이 얻고자 하는 결과의 형태를 확실하게 정의하는 것이 중요합니다. 그리고 그 결과의 최선값을 얻기 위해서는 어떤 구조를 가져야할지, 그리고 어떤 parameter들로 세팅해야할지를 정하는 것이 중요합니다. 그런데 이 과정들은 데이터에 대한 전반적인 이해가 필수적이고, 그 이해를 바탕으로 구조의 부분부분들이 하는 역할을 확실하게 파악해서 모델링하는 것이 어려운 작업인것 같습니다. 그리고 모델들이 점점 더 발전하면서 이런 기초적인 모델보다는 CNN의 모델 부분과 결합해서 사용하기도 하고, RNN과 CNN의 일정 부분끼리의 조합을 통해 더 좋은 시너지를 내기도 합니다.  
이에 발맞춰서 나아가려면 두 모델의 구간구간별로 하는 역할이나 장점들, 그리고 주의점들을 완벽하게 파악해야할 것이고, 여기서부터 새로운 모델을 만들어내거나 생각해내는 시작점이 된다고 생각하면서 포스팅을 마치겠습니다.  
아 그리고 위 모델에 대한 full code는 https://github.com/wujincheon/NCsoft/tree/master/graph_team 에 저장해놓았으니 참고하시면 됩니다.

---

## 다음 포스팅에는..

Sequence to Sequence로 생각할 수도 있는 구조인 LSTM AutoEncoder에 대해서 기본 개념과 코드 적용을 해보겠습니다.
