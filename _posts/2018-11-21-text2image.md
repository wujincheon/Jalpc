---
layout: post
title:  "Paper Review - Generative Adversarial Text to Image Synthesis"
date:   2018-11-21
desc: "Generative Adversarial Text to Image Synthesis"
keywords: "Text to Image, GAN, Joint Embedding"
categories: [Deep learning]
tags: [Text to Image, GAN, Joint Embedding]
icon: icon-html
---

이번 포스팅에서는 Text to Image를 다룬 연구 중에서 GAN을 활용한 논문인 'Generative Adversarial Text to Image Synthesis' - Scott Reed. et al,2016 에 대해서 리뷰를 해보겠습니다.  


## 1. Introduction - Purpose of the paper
>  
이 논문의 목적은 사람이 쓴 문장을 기반으로 해서 그를 잘 반영하는 실제같은 이미지를 합성해내는 모델을 제안하고 성능을 비교하는 것입니다.  

![](https://i.imgur.com/sIZ6zpk.png?1)

 
##### Text에서 중요한 시각적 요소들 학습하기 
 - 문장이 입력되면, 문장을 보고 어떤 단어들을 중점적으로 학습할지, 그리고 어떤식으로 그 단어를 만들어낼지 학습하는 것이 중요합니다. 왼쪽의 문장만 보면, 작은 새의 특징이 나와있는데 사실 문장만 보고 떠올릴 수 있는 사진들은 매우 많습니다. 그러나 대부분의 사람들이 떠올리는 중심 물체인 새에 대한 묘사는 비슷할 것입니다.  
 
##### 학습을 기반으로 이미지 합성하기 
 - 위의 그림처럼 이미지와 그에 대한 설명이 달린 데이터를 이용했고, 텍스트로부터 새로운 이미지를 합성하는 일이므로, 실제의 이미지를 기반으로 해서 합성된 이미지가 실제처럼 만들어지기 위해 GAN의 학습 방법을 이용했다고 합니다. GAN에 대해서는 조금 후에 간단히 설명하도록 하겠습니다.

 
## 2. Related Work 
>  
텍스트를 이미지로 바꾸는 직접적으로 관련있는 선행연구는 없지만, 간접적으로 연관된 연구들을 논문에서 소개하고 있습니다. 논문에서 소개한 관련 연구들을 하나씩 살펴보겠습니다.

##### 1. Multimodal Prediction 문제 연구
- 하나의 텍스트로부터 생각해낼 수 있는 이미지는 많기 때문에, 합성될 수 있는 이미지는 매우 많습니다. 즉, 최적으로 판단할 수 있는 modal이 존재하는 것이 아니기 때문에, 어떤 것을 선택해야 최적일지에 대한 문제 해결방법이 연구되고 있습니다.

##### 2. 여러 modal에서 공유되는 representation 학습 연구 
- 하나의 텍스트로부터 만들어질 수 있는 이미지가 여러개라고 하더라도, 텍스트가 담고있는 내용, 물체나 사람에 대해서는 유사하게 이미지에 담기게 될 것입니다. 이런 경우에 여러 이미지에서 공유되는 것이 무엇인지, 그것을 학습하는 연구도 진행되고 있다고 합니다.

##### 3. Deep Convolutional decoder network 연구
- 이미지를 만들어내는 과정이 decoder에 해당하는데, DCN(Deep Convolutional Network)을 기반으로 실제같은 이미지를 합성하는 연구들도 진행되고 있습니다.

##### 4. Image to Text 연구
- 이 논문의 목적과는 반대로 이미지에서 사물의 특징들을 인식해낸 후 그 내용을 텍스트로 작성해내는 모델들이 연구되었다고 합니다.

##### 5. 이미지의 class label을 기반으로 GAN을 학습시켜 이미지를 생성하는 연구
- GAN을 학습시킬때, 이미지의 class를 기준으로 학습시켜서 class정보를 기반으로 하는 이미지를 생성해내는 연구들이 있다고 합니다.

-> 여기서 착안해서 이 논문에서는 class label 대신에 text description을 기반으로 이미지를 합성해내는 GAN을 만들 것이라고 제안하고 있습니다. 그리고 Recurrent autoencoder를 이용해서 text to image에 대한 모델이 연구되긴 했지만, 상당히 비현실적인 이미지들이 만들어졌다고 합니다. 
 결론적으로, 이 논문에서는 텍스트 인코더를 통해서 텍스트 정보를 담고, 그 정보와 매칭되는 이미지를 이용해서 GAN을 학습시킵니다. 그 후에 텍스트가 주어졌을때 새로운 그럴듯한 이미지를 만들어내는 것이 목적입니다.


## 3. Background
>  
이 논문에서는 모델을 제시하기 전에, 알고 있어야할 background 두 가지를 소개하고 있습니다.


#### 3-1. GAN(Generative Adversarial Network) - 생산적 적대 신경망

![](https://i.imgur.com/IODZANV.png?1)

 
GAN은 적대하는 네트워크 즉, 네트워크 속에 적대하는 무언가가 있다는 의미에서 유래되었다고 합니다. 이 네트워크안에는 D(Discriminator), 그리고 G(Generator)가 있는데 Generator는 이미지를 생성하려고 하고, Discriminator는 그이미지를 실제 이미지와 비교하면서 진짜인지 가짜인지 분류해내려고 합니다. 이렇게 Discriminator와 Generator가 번갈아가면서 학습을 하게되는데, 위의 그림을 보시면 이해가 잘 될것입니다. 처음에는 실제 real 데이터의 분포와는 매우 다른 형태의 fake 데이터를 만들게 되는데, 처음에는 그래서 Discriminator도 구분을 잘하게 됩니다.  

이 결과를 바탕으로 Generator는 좀 더 real에 가까운 데이터를 생성하게 되고, Discriminator는 다시 이를 구분하게 됩니다. 계속해서 Generator가 수많은 학습을 거치게 되면 실제와 유사한 분포의 데이터를 생성하게 되는데, 그에 다다르면 Discriminator도 구분을 잘 하지 못하게 됩니다. 이때의 Generator의 결과물은, 이미지로 치자면 실제와 유사한 이미지를 생성해낼수 있게 되는 것입니다.

GAN을 정의해내는 식을 보면서 학습과정을 다시 설명하겠습니다. 이 식을 minimax 문제라고 불리우는데 왜 그런지 살펴볼게요.
우선 Discriminator의 관점에서는 실제 데이터를 1로 구분하고 Generator가 만들어내는 G(z)는 0으로 구분하면, 오른쪽 term에서 log 안의 값을 1에 가깝게 만드려고 하기 때문에, V값은 최대화되도록 설계된 식입니다.
반대로 G의 입장에서는 D가 G(z)를 1로 판별하도록 생성하게 되는데 그러면 오른쪽 term이 마이너스 값을 크게 가지게 되기 때문에, 전체 V값이 감소하게 됩니다. 최대한 1로 만들어낼수록 V값이 줄기 때문에 G의 관점에서는 V를 minimize하는 것입니다.


#### 3-2. DC-GAN(Deep Convolutional GAN)  

Dcgan은 아래와 같은 구조로되어 있는데, Gan이 가지고 있는 기본적인 메커니즘은 같습니다. 그러나 gan의 단점을 보완한 모델로, 
일반적으로 GAN은 학습이 잘 안되는 불안정성이 큰데 이를 그나마 극복할 수 있게 만들어낸 구조입니다. 먼저 Generator와 Discrminator에 모두 Batch Normalization을 이용했고, max pooling 대신 convolutional net을 이용합니다. 그리고 fully connected layer를 제거하는 등의 변화를 적용했다고 합니다. 

![](https://i.imgur.com/7dsaV32.png?1)  


. 


#### 3-3. Deep symmetric structured joint embedding

> Text를 encoding하여 얻는 것과 Image를 encoding하여 얻는 것이 같은 임베딩 벡터를 학습하는 것입니다.

![](https://i.imgur.com/dadhaxx.png?1)  

 학습과정에서 발생하는 loss는 위 식과 같은데, 앞의 term이 실제 라벨y와 이미지 분류기에서 나온 값 사이의 로스를 나타냅니다. 여기서 이미지 분류기에서 나온값은 이미지를 넣었을때 그에 가장 잘맞는 텍스트를 의미합니다. 이에대한 자세한 내용은 아래식을 보면 알수 있습니다.

![](https://i.imgur.com/tFzQRMr.png?1)  

 위의 식이 이미지분류기에 대한 식입니다. V는 이미지로 특정 이미지를 넣었을때 인코딩된 벡터가 나옵니다. 그러면 모든 라벨값, 즉 모든 텍스트에 대한 인코딩값을 하나씩 내적하면서 argmax값을 도출해내게 되는데, 이것이 v라는 이미지를 가장 잘 나타내는 text값으로 표현되는 것입니다. 마찬가지로 텍스트 분류기도 생각해낼 수 있습니다. 


- 다시 loss로 돌아가면, 즉 두 분류기에서 특정이미지와 텍스트에 대해 서로를 잘 매칭하도록 loss값을 최소화하려는 것입니다.

![](https://i.imgur.com/RRmXnGT.png?1)  

- 위의 그림에서처럼 이미지의 값이 인코딩되고, 텍스트의 값이 인코딩되어서 둘의 매칭되는 정도의 스코어를 산출해내게 되는데 문장에 적절한 맨 아래그림의 loss가 0에 가까운것을 볼 수 있습니다.  



## 4. Method
>  
이 논문에서 제안하는 Text to Image의 모델 설계에 대해서 알아보겠습니다.

#### 4-1. Network architecture

> Convolutional RNN으로 text를 인코딩하고, noise값과 함께 DC-GAN을 통해 이미지 합성해내는 방법을 제시했습니다.

![](https://i.imgur.com/PCp0FiD.png?1)  

논문에서는 기존의 DC-GAN을 이용하되 처음 인풋단에서 다르게 진행이 됩니다.
먼저 Generator 단에서는 텍스트를 정량화시키기 위해서 텍스트 인코더를 통해서 임베딩 과정을 거칩니다. 그리고 인코딩된 벡터, 사진에서 파란색 길다란 박스모양으로 만들어지면, fully-connected layer를 이용해서 저차원으로 축소시킵니다. 그리고 랜덤한 noise와 concat하는 과정을 통해서 인풋을 만듭니다.

그 다음은 deconvolutional network에 따라 generating 과정을 진행시킵니다. 
그리고 Discriminator 단에서는 stride를 2로 설정하여 반복 진행을하고, 4X4 convolution이 되면 앞단에서 썼던 텍스트 벡터(인코딩 후 차원 축소)를 복사해서 concat을 다시 하게 됩니다. 그로부터 끝단에서 final score를 구해서 분류를 하게되는 구조를 제안했습니다.

#### 4-2. GAN-CLS (matching-aware Discriminator)

- Real Image / matched text
- Real Image / Mismatched text
- Fake Image / Any text

기존의 naïve GAN 에서는 이미지가 가짜인지 진짜인지만 구별하기에 두개의 타입으로만 구분했지만, 이 모델에서는 그 이미지가 인풋에서 들어간 텍스트와 매칭이 되는지가 중요한 요소이기 때문에, 위와 같이 세개의 타입으로 구분해서 스코어링을 제시했습니다. 실제 이미지에 매칭되는 텍스트, 실제이미지지만 맞지않는 텍스트, 이미지가 가짜인 것으로 분류하고 있습니다.

GAN-CLS에서는 앞에서 설명한 구조와 같이 먼저, 텍스트를 인코딩하고 noise와 같이 fake image를 생성을 하게 됩니다. 그리고 위의 세가지 타입의 score를 정의해서, 실제의 이미지에 텍스트까지 매칭되는 경우가 되도록 반복해서 학습을 하게하는 구조입니다. 여기서 두번째와 세번째 모두 결과적으로는 틀리는, fake로 학습을 시키는 것이 포인트입니다.


#### 4-3. GAN-INT (Learning with manifold interpolation)


Manifold 형태의 데이터 구조에서 텍스트나 이미지의 임베딩된 여러 개의 쌍들사이를 메우는, Interpolate하는 부분을 학습하는 것은 중요하다고, 논문에서 말하고 있습니다. 그를 통해서 수많은 새로운 텍스트 임베딩들을 만들 수 있기 때문입니다. 이렇게 새로운 임베딩들에 대해서는 labeling cost가 발생하지 않기 때문에 효율적이라고 할 수 있습니다. 다시 말하면, 비슷한 이미지에 대해서 수많은 텍스트 임베딩을 만들면 그들이 공유하는 무언가를 찾아낼 수 있기 때문에 조금 더 그럴듯한 이미지를 만들 수 있다는 것입니다.  

![](https://i.imgur.com/nozxPV2.png?1)  

위의 식은 GAN의 목적식인데 여기에 주황색 플러수 부분의 식을 추가하면 된다고 합니다. 전혀 다른 카테고리의 다른이미지로부터의 텍스트 두개를 동시에 고려하면서 noise와 함께 generate를 하게되면, 두 개의 텍스트의 중간, gap을 채우는 이미지를 만들어낸다고 이해를 하면 될 것 같습니다. 그 후 새로 만들어낸 텍스트와 매칭이 되는지 안되는지 확인을 통해 더욱더 실제같은 이미지를 생성해내도록 설계되있습니다.

#### 4-4. Inverting the generator for style transfer

![](https://i.imgur.com/jdnMwhc.png?1)  


마지막으로는 generator를 거꾸로해서, noise 부분을 이끌어내는 부분을 설명하고 있습니다. 즉 텍스트가 담고 있는 주 사물에 대한 내용이 아니라, 그 외적인 부분을 스타일이라고 부르는데 배경색이나, 사물의 포즈나 부분적인 요소들을 의미합니다.
위 식에서 S는 스타일 인코더로, 생성해낸 이미지를 스타일 인코더로 인코딩하고, 그 값과 실제로 인풋에 들어갔던 노이즈와의 2norm 제곱값을 로스로 설정해서 학습을 하는 것을 보여줍니다. 이로써 이미지에서 스타일 부분만을 따로 구분해서 다른 이미지를 생성할때 전달할 수 있게 된다고 합니다.


## 5. Experiments
>  
논문에서 제시한 방법론들을 기반으로한 실험 설계와 결과를 살펴보겠습니다.

#### 5-1. Dataset 및 setting

##### Dataset
- CUB dataset - 11,788 개의 새 이미지
- Oxford-102 dataset - 8,189 개의 꽃 이미지

##### Encoder
- Text encoder – deep convolutional recurrent text encoder (1,024-dimensional)
- Image encoder – 1,024-dimensional GoogLeNet

##### Hyper Parameter
- Image size : 64 X 64 X 3
- Learning rate : 0.0002
- Adam solver with momentum 0.5
- Minibatch size : 64
- 600 epochs training


#### 5-2. Qualitative results

![](https://i.imgur.com/WBmKiqa.png?1)

먼저 CUB dataset에 있는 만여개의 새 이미지를 이용해서 기존의 GAN과 논문에서 제시한 세 모델을 비교한 그림입니다. 가짜 이미지의 type을 세개로 분류한 GAN-CLS와 기존의 GAN은 새의 색이나 전체적인 실루엣의 느낌은 비슷하게 가져가지만, 이미지가 상당히 비현실적인것을 알 수 있습니다. 이에 반해 interpolation을 적용한 아래의 두 이미지는 상당히 현실적인 이미지를 가져오는 것을 알 수 있고, 이로부터 interpolation을 통해 이미지를 합성할때 다른 이미지들로부터 그럴듯한 것을 추정하는 방법이 중요하다는 것을 보여주고 있습니다.


#### 5-3. Disentangling style and content

- Style Encoder
- K-means clustering
- Predicting style variables  

Caption에서 나타나는 내용들은 주 사물에 대한 내용이 대부분이기 때문에 그럴듯한 이미지를 만들어내기 위해서는 noise를 잘 만들어내고 구분해내야 합니다. 
그래서 앞서 소개했던대로 기존의 과정을 거꾸로하는 스타일인코더를 이용해서 노이즈를 구분해내도록 실험했다고 합니다. 구분해낸 스타일들을 100개의 클러스터로 k-means clustering을 거쳤고 클러스터 안의 스타일들은 유사한 스타일이라고 정의했습니다. (새의 이미지의 경우, 배경의 색, 그리고 포즈에서는 부리, 배, 가슴, 꼬리 등등을 분할해서 정의했습니다.)


![](https://i.imgur.com/USe0fpN.png?1)  


위에서 제시했던 네가지 모델에 대해, 스타일인코더를 적용해서 스타일을 예측하는 실험을 해본 결과입니다. 여기서도 보다시피 확실하게 알수없는, 즉 새로운 이미지를 만들어낼때 , interpolation을 적용한 모델들이 비교적으로 우월하다는 것을 한번더 확인할 수 있었습니다.
이로부터, 원래는 캡션만으로는 알수 없었던 배경이나 부분적인 내용들을 이 학습 과정의 결과를 통해서 대입할 수 있게 되고, 그로부터 그럴듯한 배경들이 나타남을 보여주고 있습니다.

#### 5-4. Style transfer

![](https://i.imgur.com/LzfdgmL.png?1)  

위 그림은, 5-3에서 스타일 인코더를 이용해 분리해낸 스타일을 다른 이미지를 생성할때 transfer해낸 실험 결과입니다.
맨 위의 이미지에서 얻어낸 스타일을 이용해서, 아래의 새로운 이미지를 만들때 비슷하게 가져감으로써 그럴듯한 이미지를 만드는 것입니다.
왼쪽의 텍스트 디스크립션만 보고서는 새에 대한 정보만 알 수 있으므로, 그럴듯한 배경을 만들어내기 어렵습니다. 여기서 위의 이미지로부터의 스타일을 tranfer 시킴으로써 그럴듯한 이미지들을 생성하고 있는 것입니다.

#### 5-5. Sentence interpolation

![](https://i.imgur.com/BSST6r8.png?1)  

첫번째 그림을 보면, blue bird에서 red bird로 변화한다는, 텍스트의 일부가 변경되었음에도 상당히 그럴듯하게 사진이 만들어졌습니다. 두 텍스트문장을 interpolate를 하게 되니 noise 즉 style 에 해당하는 부분은 고정되기 때문에, 배경은 유사하게 유지가 되고 새의 색만 바뀌도록 이미지가 형성된 것입니다.  

오른쪽의 경우는 텍스트 임베딩값을 interpolate하는 것처럼 임의의 두 noise 벡터를 interpolate한 결과입니다. 왼쪽과는 반대로 텍스트 부분은 고정되기 때문에 주요 사물에 대한 정보는 그대로 보존되면서, 두 이미지로부터 만들어진 noise, style이 smooth하게 연결되는 것을 확인할 수 있다고 합니다.

#### 5-6. MS-COCO dataset 적용

![](https://i.imgur.com/0grSdhE.png?1)

위에서 이용한 파라미터들과 DC-GAN의 구조를 그대로 이용하여 MS-COCO데이터에 일반화시키는 실험을 진행했다고 합니다.  
이 그림들이 그 결과인데, 나름 텍스트들이 전달하고자 하는 것들에 대해서 그림이 그럴듯하게 나오는 것을 확인할 수 있습니다. 주요 객체들이나, 주위 배경색, 부분적인 요소들도 나름 그럴듯하게 만들어냅니다. 그러나 오른쪽위에 투수가 공을 던지는 사진의 경우, 중심이 되는 객체가 여러 개인 경우인데, 사진들이 상당히 비현실적으로 만들어짐을 알 수 있습니다. 야구에 대한 텍스트임에서 경기장의 모양이나 선수들의 옷색 등은 비슷하게 가져가지만, 객체들의 포즈를 정확하게 캐치하지 못하고 있음을 이야기하면서 이 부분에 대해서는 추후 연구가 필요하다고 이야기하면서 실험을 마치고 있습니다.

## 6. Conclusions

- ##### 시각적인 정보를 담는 text를 기반으로 이미지를 합성하는 간단하고 효율적인 모델 제안
- ##### Interpolation을 통해 향상된 성능을 보이도록 제안
- ##### 주요 사물과 주변 배경을 분리하고, 그를 복사해내는 모델 제안
- ##### MS COCO 데이터에도 일반적으로 적용될 수 있는 모델임을 실험적으로 증명

### "고해상도 이미지를 만들고, 많은 텍스트를 반영하기 위한 추후 연구 필요"

## 마치며..

이미지로부터 어떤 물체들이 인식이 되는지, 이미지에 어떤 특성이 있는지 문장으로, 텍스트로 만들어내는 연구들은 많이 진행되었고 좋은 성능을 보이는 경우도 많이 있는 것으로 알고 있습니다. 그러나 그 반대의 과정을 좀 더 좋은 성능을 내기 위해 여러 실험들을 진행한 것이, 그리고 실제 결과로 나타난 것이 이 논문의 장점이라고 생각합니다. 이후에도 더 발전된, 더 좋은 성능을 보이는 모델들이 많이 연구되었기에, 이 논문을 통해 이해한 text to image의 기본 메커니즘을 바탕으로 다른 논문들도 살펴보겠습니다.
