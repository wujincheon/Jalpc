---
layout: post
title: "AutoEncoder - LSTM AutoEncoder (Sequence to Sequence)"
date: 2018-12-28 
desc: "AutoEncoder - LSTM AutoEncoder (Sequence to Sequence)"
keywords: "RNN, LSTM, Sequence to Sequence, AutoEncoder, tensorflow"
categories: [Deep learning]
tags: [RNN, LSTM, Sequence to Sequence, AutoEncoder, tensorflow]
icon: icon-html
---

이번 포스팅에서는 저번 RNN 포스팅에 이어서 LSTM AutoEncoder에 대한 개념을 Sequence to Sequence의 구조와 함께 살펴보겠습니다. 오토인코더에 대한 간단한 설명과 이를 tensorflow 코드로 구현해보면서 왜 seq2seq의 구조를 가진다고도 할 수 있는지 설명해보겠습니다.  

---

## 1. AutoEncoder  
##### 오토인코더란?  
![](https://i.imgur.com/aNFHthO.png?1)  
그림에서 한 눈에 알 수 있듯이, 오토인코더란 주어진 인풋인 x와 아웃풋인 z가 유사하도록 인코딩과 디코딩이 학습되는 뉴럴 네트워크 구조입니다. 본 그림은 가장 간단한 구조로써, Encoder는 주어진 인풋에 대해 원래의 차원보다 적은 수의 차원으로 표현합니다. 이 과정을 인코딩이라고 하며, 원래 차원의 데이터를 저차원으로 줄여준다고 생각할 수도 있습니다. 그러나 인코딩이 제대로된 차원축소의 개념을 가지려면, 디코딩도 적절하게 학습되어야 합니다. 줄어든 hidden layer의 값들을 다시 인풋으로 하여 원래의 차원대로 다시 만들어주는 과정을 디코딩이라고 하는데, 다시 만들어낸 아웃풋값들이 인코더에 들어가는 인풋값들과 유사하도록 디코딩이 이루어져야 합니다.  
그래서 반복적인 학습을 통해 인풋과 아웃풋이 매우 유사하도록 학습된 뉴럴네트워크의 hidden layer는 본 데이터의 차원을 축소한 값이라고 해석할 수 있습니다. 실제로 매우 높은 차원의 데이터에 대해서 저차원으로 임베딩할 때 오토인코더를 다양하게 활용하고 있습니다. 왜냐하면 높은 차원의 데이터를 적은 차원으로 표현할 수 있다는 것이기 때문에, 본 데이터의 특징을 잘 파악해서 추출하는 의미로 해석할 수 있기 때문입니다.  

##### 오토인코더의 학습과정  
![](https://i.imgur.com/sw2ktNB.png?1)  
먼저 입력하고자 하는 input값에 대해서 인코딩을 진행할 가중치와 함수가 필요합니다. 다양한 initialization 방법 중 하나를 택하여 인코더의 가중치를 초기화한 후, 그 가중치와 설정한 함수 및 bias를 통해 y값을 얻어냅니다. 위의 그림을 통해 이야기하자면, 5개의 인풋에 대해 가중치를 다르게 주면서 차원이 축소된 3개의 아웃풋을 얻게 되는 것입니다. 그리고 아직 이 단계에서의 아웃풋은 본 데이터를 축소한 차원의 값이라고 할 수는 없고, 이후에 진행되는 학습을 통해 보정됩니다.  

![](https://i.imgur.com/JNQuDk0.png?1)  
위에서 얻은 y값을 바탕으로 이번에는 디코딩 과정을 거치게 됩니다. 디코더에서도 마찬가지로 가중치와 bias값을 통해서 원래 데이터의 차원으로 아웃풋을 계산하게 됩니다. 아마 인코더와 디코더에서 처음에 설정되어 있는 가중치는 임의의 값으로 볼 수 있기 때문에, 아웃풋으로 나온 z값은 처음에 인풋으로 들어간 x값과 매우 다를 것입니다. 이제 여기서 두 값을 같도록 학습하기 위해, x값과 z값의 차이를 loss function을 정의해주고 Gradient descent 방법이나 RMSprop, Adam optimizer 등의 method로 학습을 시키면 두 값이 같아지도록 하는 인코더와 디코더의 가중치와 bias들을 얻어낼 수 있습니다.  
저는 중간의 hidden layer에서 사용할 cell을 LSTM cell을 이용할 것인데, 인풋들이 시간의 흐름에 따른 시계열 데이터를 사용했기 때문입니다. 그래서 시간의 영향을 잘 반영하면서 본 값을 복원해주는 오토인코더를 만들어내도록 모델링을 해보았습니다.

##### 오토인코더의 여러 종류  
오토인코더는 목적에 따라, 그리고 데이터에 따라 다양한 방식으로 tuning되어 사용되는데 그 기반이 되는 몇 가지 종류에 대해서 wikidocs의 그림과 자료를 활용하여 정리해보겠습니다.  

**Staked AutoEncoder**  
![](https://i.imgur.com/MmkQ5FE.png)  
위 그림은 인코더와 디코더가 단층이 아니고 여러층으로 이루어진 오토인코더를 의미합니다. stacked autoencoder와 기본 autoencoder의 큰 차이점은 학습 방법이 다르다는 것입니다. 일반 오토인코더는 Loss값을 최소화하는 방향으로 역전파를 통해서 weights를 학습합니다. 그러나 stacked autoencoder는 워낙 구조가 복잡하기 때문에 역전파를 할때 gradient가 제대로 전달되지 않는 문제들이 발생한다고 합니다. 그래서 층별로 학습을 하고 전체적으로 다시 tuning을 하는 방식, Greedy Layer-Wise Training 처럼 학습을 한다고 합니다.  

**Denoising AutoEncoder**  
![](https://i.imgur.com/cYbSzTu.png)  
Denoising AutoEncoder는 인풋 데이터에 노이즈를 추가하고, 아웃풋 데이터를 노이즈가 없는 데이터로 설정하여 노이즈를 걷어내는 모델이라고 생각하시면 됩니다. 즉, 데이터가 주어졌을 때 그 데이터의 노이즈를 제거하여 원래 데이터의 특징을 뽑아내는 모델인 것입니다. 학습 방식은 기본적인 구조의 오토인코더와 유사하지만, 이 모델의 목적은 주어진 데이터에 대해서 robust하게 그 특징을 추출해낼 수 있는 방법을 제안하는 것입니다. 예로 들자면, MNIST와 같은 숫자 이미지를 판별하는 모델을 만들때, 주어진 데이터에 대해서 노이즈를 추가하면서 라벨을 유지시킨 데이터들을 학습에 이용하면, 해당 class에 대해 노이즈가 섞인 데이터가 입력되어도 예민하게 반응하지 않도록 돕는 것입니다.  

**Variational AutoEncoder**  
![](https://i.imgur.com/x16sCIy.png)출처 : 핸즈온 머신러닝  
이 모델은 위의 그림에서 보는 것처럼 인코더와 디코더 사이에 다른 부분이 있습니다. 인코더를 통해 인풋을 인코딩한 값들로부터 평균 코딩과 평균 표준편차를 만드는 것입니다. 그렇게 해서 얻어낸 분포는 평균이 u이고 표준편차가 시그마인 가우시안 분포를 따르게 되고, 이 가우시안 분포로부터 랜덤하게 다시 샘플링해서 얻은 값으로 디코딩을 하는 것입니다. 그래서 VAE는 학습이 모두 종료되어도 같은 인풋에 대해 다른 아웃풋을 낼 수도 있기 때문에 확률적 오토인코더라고 부릅니다. 또한 인풋을 인코딩한 deterministic한 값이 아니고 랜덤해서 샘플링한 새로운 값을 이용하기 때문에 생성 오토인코더라고 부르기도 합니다.  
VAE는 실제와 유사하게 만들어내는 알고리즘인 GAN에 비해 학습을 안정적으로 시킬 수 있고, 저차원의 벡터인 representation도 학습시킬 수 있는 장점이 있다고 합니다. 그리고 오버피팅 방지에도 어느정도 도움이 된다고 개인적으로 생각합니다.

## 2. LSTM AutoEncoder 구현  

## 1. 모델 구현 준비  
##### 모듈과 스케일링 함수 및 parameter 정의
```python
import tensorflow as tf
from tensorflow.python.ops.rnn_cell import LSTMCell

import numpy as np
import pandas as pd
import random as rd
import time
import math
import csv
import os
from sklearn.preprocessing import scale

tf.set_random_seed(2016)
batch_num = 1
hidden_num = 15
step_num = 25
elem_num = 385
iteration = 3205

data=sklearn.preprocessing.scale(data)
```
- 텐서플로에서 학습을 할 때 시드값을 정해주지 않으면 학습시킬때마다 초기값이 다르게 설정되어 결과가 다르게 나올 수 있습니다. 그리고 제가 적용한 시계열데이터는 변수마다의 스케일이 매우 다르기 때문에, 평균을 중심으로 데이터를 scaling하는 sklearn의 패키지를 사용했습니다. 그리고 15개의 cell을 이용하기 위해 설정을 해주었고, 과거 25일을 연속적으로 반영하는 representation을 얻기 위해 사이즈를 25로 설정했습니다. 이 부분은 뒤에서 다시 그림과 함께 설명하겠습니다.  
- 제가 사용한 데이터는 앞선 포스트와 같이 변수가 약 350개, record가 약 3000여개의 데이터로, 25의 사이즈로 1씩 sliding하며 3205번 트레이닝 시켰습니다.  
![](https://i.imgur.com/vj9BZ7l.png?1)  

##### 인풋 데이터 정의
여기서도 many to many와 마찬가지로, 3000개의 정보를 한번에 학습시키는 것이 아니라 일정 길이의 인풋씩 잘라서 학습을 시켰습니다. 

```python
for i in range(iteration):
        input1 =data[i:i+25,]
        input1=input1.reshape([1,25,385])
```
위의 코드를 보면 알 수 있듯이, 저는 seq_length 즉 25일씩 학습을 시켰습니다. 아래 사진처럼 day1~25를 넣어서 day1~25을 아웃풋으로 하여 그 둘이 같아지도록 학습한 것입니다. 25일치의 학습이 끝나면 hidden layer cell 중 그 마지막을 25일의 흐름을 반영하는 25일의 대표값으로 정했습니다. 히든 레이어에 해당하는 값을 사용해서 고차원의 데이터를 저차원의 임베딩된 벡터로써 표현함과 동시에, 25일의 흐름을 연속적으로 학습한 마지막 결과임을 얻게 됩니다.  

![](https://i.imgur.com/k0kRKNj.png?1)
 
---

## 2. 텐서플로우를 통한 LSTM AutoEncoder 구성  
##### LSTM 네트워크 구성  
```python
class LSTMAutoencoder(object):

    
    def __init__(
        self,
        hidden_num,
        inputs,
        cell=None,
        optimizer=None,
        reverse=False,
        decode_without_input=False,
        ):
        

        self.batch_num = inputs[0].get_shape().as_list()[0]
        self.elem_num = inputs[0].get_shape().as_list()[1]
# (a)
        if cell is None:
            self._enc_cell = LSTMCell(hidden_num)
            self._dec_cell = LSTMCell(hidden_num)
        else:
            self._enc_cell = cell
            self._dec_cell = cell
#(b)
        with tf.variable_scope('encoder'):
            (self.z_codes, self.enc_state) = tf.contrib.rnn.static_rnn(self._enc_cell, inputs, dtype=tf.float32)
#(c)
        with tf.variable_scope('decoder') as vs:
            dec_weight_ = tf.Variable(tf.truncated_normal([hidden_num,
                    self.elem_num], dtype=tf.float32), name='dec_weight'
                    )
            dec_bias_ = tf.Variable(tf.constant(0.1,
                                    shape=[self.elem_num],
                                    dtype=tf.float32), name='dec_bias')

            dec_state = self.enc_state
            dec_input_ = tf.zeros(tf.shape(inputs[0]),
                    dtype=tf.float32)
            dec_outputs = []
            for step in range(len(inputs)):
                if step > 0:
                    vs.reuse_variables()
                (dec_input_, dec_state) = \
                    self._dec_cell(dec_input_, dec_state)
                dec_input_ = tf.matmul(dec_input_, dec_weight_) \
                    + dec_bias_
                dec_outputs.append(dec_input_)
            if reverse:
                dec_outputs = dec_outputs[::-1]
            self.output_ = tf.transpose(tf.stack(dec_outputs), [1,
                    0, 2])
#(d)
        self.input_ = tf.transpose(tf.stack(inputs), [1, 0, 2])
        self.loss = tf.reduce_mean(tf.square(self.input_
                                   - self.output_))
        global_step = tf.Variable(0, trainable=False)
        starter_learning_rate = 0.1
        learning_rate = tf.train.exponential_decay(starter_learning_rate,
                                             global_step,
                                             100, 0.9, staircase=True)
#(e)
        if optimizer is None:
            self.train = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)
        else:
            self.train = optimizer.minimize(self.loss)
```
LSTM 오토인코더의 코드를 차근차근 살펴보면, 크게 cell을 정의하고 인코더 정의, 디코더를 정의하고 loss function과 학습률 그리고 optimizer를 정의하는 순서임을 알 수 있습니다. (a) 를 보면 먼저 인코더와 디코더의 LSTM Cell을 몇개로 설정할지 정하게 됩니다. (b) 그리고 인코더를 지정한 사이즈의 cell로 만들어냅니다. (c) 에서 디코더로 가중치와 bias를 설정해줌으로써 인풋과 아웃풋을 정의해줍니다. 인코더에서의 아웃풋을 디코더에서의 인풋으로 설정하고 계산할 수 있도록 조정합니다. (d)에서는 처음의 인풋과 디코더에서의 아웃풋 사이의 차이를 loss function으로 설정해주고 학습률을 0.1에서 점점 감소하도록 decay함수를 통해 설정해줍니다. 이렇게 해주면 학습이 충분히 되었을때 갑자기 발산하거나 튀어버리는 문제가 발생하는 것을 막아주는 역할을 한다고 합니다. 그런 후에 (e) 다양한 optimizer중에서 RMSProp Optimizer를 학습 메커니즘으로 설정해 놓습니다.
  


---

## 3. 본격적인 학습  
LSTM 오토 인코더를 정의하고 25일의 sequence를 인풋으로 전체의 데이터에 대해서 1씩 sliding 하면서 학습을 진행합니다. optimizer를 통해 loss를 기준으로 학습을 시키면서 매 학습마다 발생하는 loss를 저장해보았습니다.
```python 
# placeholder list
p_input = tf.placeholder(tf.float32, shape=(batch_num, step_num, elem_num))
p_inputs = [tf.squeeze(t, [1]) for t in tf.split(p_input, step_num, 1)]


a=[]
cell = tf.nn.rnn_cell.LSTMCell(hidden_num, use_peepholes=True)
ae = LSTMAutoencoder(hidden_num, p_inputs, cell=cell, decode_without_input=False)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    
    for i in range(iteration):
        input1 =data[i:i+25,]
        input1=input1.reshape([1,25,385])

        loss, _=sess.run([ae.loss, ae.train],  feed_dict={p_input: input1})
        a.append(loss)

        print('iter %d:' % (i + 1), loss)
    
    (input_, output_) = sess.run([ae.input_, ae.output_], {p_input: input1})
    print('train result :')
    print('input :', input_[0, :, :].flatten())
    print('output :', output_[0, :, :].flatten())
```    
![](https://i.imgur.com/jEHWMp8.png)  
loss의 변화를 체크한 것으로, 중간에 데이터의 특성때문에 loss가 크게 뛰는 것 같지만 최종적으로는 다시 loss가 잘 수렴되어서 학습됨을 확인할 수 있었습니다.
![](https://i.imgur.com/QWdMLjv.png)  
결과를 보면 loss에 대해 충분히 학습된 것을 알 수 있고, 총 385개의 정보에 대해서 모두 0.2 미만으로 차이나는 것을 알 수 있습니다. 

##### parameter 조정  
Loss를 조금 더 떨어뜨리고 안정화시키기 위해서 parameter들을 다양하게 조정하면서 실험을 해볼 수 있습니다. cell 내부의 dimension을 확장시키거나, optimizer를 바꿔보거나, 몇개의 시퀀스씩 반영시킬지, 학습률을 바꿔보는 등 조정을 해볼 수 있습니다. 

---

## 4. 결론  
인풋이 주어졌을때 그를 똑같이 만들어내는 구조의 모델을 통해, 그 중간에서 어떤 구조가 작용하고 있는지 파악할 수 있습니다. 그러면 그 중간과정이 결국은 원래의 데이터를 조금 더 간단히 표현하는, 필요한 정보들을 추출해서 나타내는 역할을 한다고도 볼 수 있습니다. 주성분 분석이나 t-SNE 등의 여러 임베딩 방법들이 있지만, 그 방법론들은 가정해야할 조건들도 다양하고 데이터의 형태에 따라 잘 작용하지 않을 수도 있는 머신러닝 기법들입니다. 그러나 이 딥러닝 기법을 활용하면 데이터의 형태에 무관하게 차원을 축소하거나 특징을 추출하는데 사용할 수 있습니다.  
그리고 오토인코더가 잘 학습한다고 가정했을때, 인코딩하기 전의 값과 디코딩 후의 값 사이의 차이를 통해서 본 데이터에서 이상치를 탐지하는 task에도 적용할 수 있다고 합니다. 단순한 구조인 오토인코더를 통해 여러 task들을 진행할 수 있고 다양한 목적에 따라 오토인코더를 다르게 활용할 수 있다는 것을 알 수 있었습니다. 다양한 오토인코더를 실제로 적용해보는 포스팅도 추후에 진행해보고 구조에 대해서 새로운 것을 생각해보는 연구도 진행할 예정입니다.

---

## 다음 포스팅에는..

Object Detection과 관련된 개념들과 논문을 살펴보겠습니다.
