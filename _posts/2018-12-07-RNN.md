---
layout: post
title: "Recurrent Neural Network(RNN)와 Long Short Term Memory(LSTM)"
date: 2018-12-07 
desc: "Recurrent Neural Network(RNN)와 Long Short Term Memory(LSTM)"
keywords: "RNN, LSTM"
categories: [Deep learning]
tags: [RNN, LSTM]
icon: icon-html
---

이번 포스팅에서는 딥러닝의 아주 기초적인 뉴럴 네트워크 구조 중 하나인 Recurrent Neural Networks(RNN)와 vanishing gradient problem을 극복하기 위해 고안된 Long Short Term Memory(LSTM)에 대해서 살펴보겠습니다. 



## 1. Recurrent Neural Networks(RNN)이란?
>  
RNN은 순차적인 정보, 시간의 순서에 따라 변하는, 데이터의 순서에 따라 결과에 영향을 주는 그러한 데이터들을 분석하는 데에 적합한 모델입니다. 단순히 어떠한 요인들에 의해 결과를 예측하거나 분류하는 기법들(회귀분석, 의사결정나무 등)과는 다르게, 과거 시점의 정보들을 순차적으로 반영해서 그 특징을 예측값에 나타내는 시계열 데이터에 적합한 것입니다.  
물론 회귀분석에도 시간에 관련된 요인을 넣어서 시계열 데이터를 분석하는 것처럼 만들 수는 있지만 메커니즘 자체가 다릅니다. RNN은 순차적으로 들어오는 정보를 차례대로 학습하고, 학습한 결과를 실제 결과와 비교해서 틀린 정보를 즉, 학습단계에 피드백을 준다는 개념으로 반복해서 학습을 해나갑니다. RNN이 학습해나가는 과정과 어떻게 시계열 데이터를 순차적으로 반영하는지 천천히 살펴보겠습니다.

![](https://i.imgur.com/kK1Kj4Y.png?1)  
 위 그림과 RNN은 데이터의 형태나 목적에 따라 여러 모델로 나뉘어질 수 있습니다. 인풋의 형태가 순차적으로 들어가는지, 하나의 인풋만 들어갈지에 따라 나뉘어지기도 하고, 여러개의 인풋이 순차적으로 들어갈때 아웃풋을 어떻게 뱉어낼지에 따라서도 구조를 다양하게 만들어줄 수 있습니다.  
- **Many to Many** : 이 이름은 첫 번째와 네 번째 구조에게 모두 붙일 수 있는 말로, 말그대로 인풋이 여러 개가 들어갔을때 아웃풋도 여러개가 나오도록 하는 형태입니다. 첫 번째 구조의 경우 각각의 인풋에 따라 그에 해당하는 아웃풋을 순차적으로 계속 뱉어내게 되고, 네 번째 구조의 경우는 인풋을 받아들이는 순서대로 학습을 해서 기억을 한뒤, 그를 바탕으로 아웃풋도 순차적으로 뱉어내는 모델입니다.  
즉, 네 번째 구조는 sequence to sequence의 구조로 볼 수도 있는데 시퀀스 투 시퀀스는 하나의 시퀀스를 받아들여서 학습을 한뒤 그에 상응하는 아웃풋 시퀀스를 내뱉는 것입니다. 예를 들자면, 영어를 한글로 번역해주는 모델이 있는데, 영어 단어들로 이루어진 문장 시퀀스를 입력했을때 그에 맞는 한국어 문장을 뱉어내주는 것입니다. 그래서 자연어 처리에 많이 쓰이는 구조라고 알려져 있습니다.  
 (many to many, sequence to sequence가 다른 구조에 비해 월등하게 많이 사용되고 다양한 형태로 연구되고 있습니다. 이 둘에 대해서는 자세히 코드를 구성하면서 프로젝트를 진행하였는데 곧 포스팅하도록 하겠습니다.)

- **Many to One** : 이 구조는 인풋이 여러 개가 들어갔는데 아웃풋이 하나만 나오는 구조로써, 예로 단어들이 주어졌을때 긍정인지 부정인지에 대한 감성점수를 뱉어주는 모델을 들 수 있습니다. 문장에 포함되는 단어들을 순서대로 인식했을때 그에 대해서, 그림상에서 가운데 동그라미들 즉, RNN cell이 계속해서 감성 점수에 대해 학습을 하게 되고 결과적으로 문장에 대한 단어들이 모두 들어가게 되면 결과를 내어주는 것입니다.  

- **One to Many** : 이 구조의 경우는 이미지처럼 하나의 인풋이 들어가면 그 이미지에 대한 설명이 문장으로, 즉 여러 개의 단어로 뱉어지게 되는 형태입니다.

One to One 즉 1대1로 뱉어지는 구조가 가장 기본적인 구조인데, 이는 너무 단순해서 굳이 뉴럴네트워크를 쓰지 않아도 더 좋은 모델들이 많이 존재하며, One to Many의 경우도 이미지를 인식해내는 더 좋은 딥러닝 기법들이 존재합니다.  
RNN, 순차적인 시계열 데이터의 속성을 파악해서 그를 반영해내는 특징을 잘 나타내는 many to many의 구조를 가장 많이 사용하며, 이를 한층이 아닌 여러층으로 쌓기도 하고, 쌍방향으로 인풋을 여러번 학습하기도 하고, cell의 특성이나 구조를 다양하게 바꾸기도 하면서, 데이터 도메인의 특성을 잘 반영하도록 활용하는 연구들이 많습니다.  
그러면 이제 RNN의 기본 구조와 학습 과정, 변형가능한 여러 구조들에 대해 간단히 알아보겠습니다.


 
 
 
## 2. RNN의 구조 및 LSTM cell? 
   
![](https://i.imgur.com/jyrYZvX.png?1)출처: ratsgo's blog  

위의 그림을 통해 인풋에 대해 어떤식으로 아웃풋이 결정되는지 어떤 의미를 가지는지 설명하겠습니다. t시점에서의 인풋 x 하나가 주어지면 그 값에 대해서 rnn cell 즉, hidden layer로 생각할 수 있는 층에서 정해진 식에 따라 계산을 해주게 됩니다. 그런데 여기서 그 시점의 데이터에 대한 가중치뿐만 아니라, t-1시점의 데이터가 주어졌을때 이용했던 t-1시점의 rnn cell에서의 가중치까지 반영하게 됩니다. 그 말은 즉, 해당 시점 이전까지의 인풋들에 대해서 학습한 cell 내의 가중치값을 해당 시점에도 반영시킨다는 것입니다. 그렇게 되면 해당 시점뿐만 아니라 과거 시점의 데이터에 대한 정보도 반영할 수 있게 되는 것입니다.  
위의 그림만 봐도 생각할 수 있듯이, 저러한 기본구조를 다양하게 바꿀 수 있음이 보일 것입니다. 인풋값과 t-1시점의 state를 반영해서 탄젠트 하이퍼볼릭 함수라는 활성함수를 쓰고 있는데, 이는 비선형함수로써 뉴럴 네트워크의 장점을 살려줍니다. 선형함수를 쓰면 그냥 선형으로 식을 입력하게 되서 굳이 hidden layer가 필요없기 때문입니다. 그래서 이 비선형함수를 sigmoid, relu 등 다양하게 바꿔서 쓸 수도 있으며, cell 자체를 LSTM cell로 바꿔줄 수 있습니다. 그럼 LSTM cell을 사용하는 이유는 무엇일까요?  


#### Long Short Term Memory Cell
LSTM cell은 말그대로 좀 더 길게, 과거의 정보들을 좀 더 오래 기억하기 위해 고안된 구조입니다.  
![](https://i.imgur.com/OXc2yio.png?1)출처: ratsgo's blog  
그림에서 윗부분에 해당하는 RNN은 단순히 인풋과 이전 시점의 state값을 통해 결과값을 내게 됩니다. 그러나 LSTM cell 내부의 구조를 보면 알 수 있듯이, 인풋과 이전 state를 받아서 시그모이드를 통해 어느 정도 과거에 대한 정보를 잊게 해주는 게이트를 만들어놓았습니다. 그 다음으로는 현재 정보를 잘 기억하기 위해 시그모이드를 통한 값과, 하이퍼볼릭탄젠트를 거친 값의 Hadamard product 연산 값을 반영하도록 설계되어 있습니다.  
이렇게 복잡한 구조를 하는 이유는 기존 RNN 구조에서 발생하는 vanishing gradient problem을 해결하기 위해서 입니다. 즉, 인풋의 길이가 길어지면 과거에 사용되었던 정보를 현재 이용할 때 문제가 생긴다는 것입니다. state를 순차적으로 학습하는데, 그 과정에서 그래디언트가 점점 줄어들기 때문에 그렇습니다. 그런데 아래와 같은 구조를 사용하게 되면, 그래디언트가 소멸되거나 매우 작아지는 것을 방지하고 좀 더 잘 기억해내기 때문에 성능이 좋아진다고 합니다.  
그러면 이번에는 그래디언트 학습이 어떤식으로 이루어지는지 간단하게 살펴보겠습니다.



## 3. 순전파와 역전파
![](https://i.imgur.com/bl7BVYg.png?1)출처: ratsgo's blog  
먼저 순전파는 말그대로 화살표대로 순서대로 계산을 하는 과정입니다. 2번 그림의 LSTM cell 내부에서 일어나는 계산들이 인풋과 이전 시점 state에 따라 어떻게 순서대로 계산되는지 나타낸 그림입니다. 이렇게 순서대로 계산을 해준 뒤에는, 실제 값과의 오차를 반영해서 학습했던 cell들을 다시 학습합니다. 이 과정이 뉴럴 네트워크를 사용하는 가장 큰 장점이기도 하면서 중요한 부분이라고 생각합니다. 실제값과의 오차로부터 거꾸로 그 오차를 전파하기 때문에 역전파라는 이름이 붙여진 것입니다.  

![](https://i.imgur.com/weOIuXi.png?1)  
오른쪽 아래에서 보이듯이 실제 y값에 대한 정보가 있을 것이고, 그에 대한 오차를 Loss function으로 정의할 수 있을 것입니다. 그러면 그 Loss에 대해서 미분을 통해, Loss에 대한 변화 즉 그래디언트를 구할 수 있게 됩니다. 그러면 그래디언트에 따라 x값을 어떤식으로 반영하고, 과거 state를 어떻게 반영할지에 대해서 재학습하게 됩니다. LSTM cell 내부에서 일어나는 각각의 계산과정마다 이 그래디언트가 거꾸로 전파될 것이고, 그 값에 따라 각 게이트들의 가중치 즉, 반영하는 정도를 수정하게 되는 것입니다.  
그래서 이 과정을 여러번 반복하게 되면, 처음에는 임의로 설정된 state 값들이 해당 데이터에 적합하도록 점점 fitting 되면서 성능이 좋은 뉴럴 네트워크가 완성되는 것입니다.





## 4. 다양한 RNN 구조
>  
위에서 언급했던 것처럼, 현재에는 RNN의 기본구조를 벗어나 엄청나게 복잡하고 다양한 구조들이 연구되고 사용되고 있습니다. 어떤 식으로 변형되어서 사용되는지 몇 가지만 맛보기로 살펴보고 포스팅을 마치겠습니다.  

#### Multilayer  
Hidden Layer를 한 층이 아닌 여러층으로 쌓음으로써 좀 더 정밀한 계산이 가능하도록 하는 구조입니다.  
![](https://i.imgur.com/Ujs3mEU.png?3)  

#### bidirection  
그림이 복잡해보이긴 하지만 원리는 간단합니다. 순차적인 인풋 x들에 대해서 양방향으로 학습을 하게 하는 구조입니다. 그렇게 해서 한쪽으로만 편향될 수 있는 결과를 좀 더 안정적으로 만들어주고자 이런 구조를 만들기도 합니다. 그러나 LSTM Cell을 이용한 모델에서는 RNN과는 다르게 일방향의 구조로만 학습이 가능하다고 해서(구조가 복잡해서? 코드상?) 두개의 layer를 만들어서 반대방향으로 학습시킨다고 합니다.  
![](https://i.imgur.com/a4dQoPH.png?1)

#### 자유롭게..  
위의 방식뿐만 아니라, 입력되는 데이터에 따라서 그를 잘 처리할 수 있도록 매우 다양하게 구조를 변형시킬 수 있습니다. 아래의 그림처럼 어떠한 문장을 문맥상에서 이해해서 감성 분석을 하고자 할때, 주어진 문장에 대해서  단어 수준에서 처리할지, 문장 수준에서 처리할지 등에 따라 여러개의 층으로 구성을 할 수 있습니다. 인풋의 사이즈 같은 것이 정해져있지도 않기 때문에 다양하게 활용할 수 있지만, 그러기 위해서는 데이터에 대한 완벽한 이해는 우선이고, 그에 맞는 여러 구조들의 장단점과 실험들이 이루어져야 할 것입니다.
![](https://i.imgur.com/BYORn1Q.gif)  
출처 : Attention based hierarchical LSTM network for context-aware microblog sentiment classification, Shi Feng et al. 2018  

## 다음 포스팅에는..

RNN의 구조 중 가장 많이 쓰이는 Many to Many의 두 개의 형태에 대해서, 실제 데이터에 적용해보면서 코드를 뜯어보며 어떻게 컴퓨터상에서 작동이 되는지 결과물과 함께 포스팅하도록 하겠습니다.
