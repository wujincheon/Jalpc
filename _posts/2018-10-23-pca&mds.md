---
layout: post
title:  "Principal Component Analysis(PCA) & MultiDimensional Scaling(MDS)"
date:   2018-10-23
desc: "Principal Component Analysis(PCA) & MultiDimensional Scaling(MDS)"
keywords: "Dimensionality Reduction,PCA,MDS,Machine Learning"
categories: [Machine learning]
tags: [PCA,MDS,Dimensionality Reduction]
icon: icon-html
---

차원축소 기법 중, Linear Embedding에 해당하는 PCA(주성분분석)과 MDS(다차원척도법)에 대한 개념과 알고리즘, 그리고 적용을 통해 자세히 알아보겠습니다. 그리고 이 자료는 고려대학교 강필성 교수님의 Business Analytics의 강의와 강의자료를 참고하였습니다.

---
&nbsp;

## 1. 차원축소란
>  
차원 축소는, 주어진 데이터의 차원을 **고차원에서 저차원으로 축소**시키는 것입니다.  
 (데이터의 의미를 나타내는 특징들을 추려내는 것이라고 생각할 수 있습니다.)  
 
 
데이터가 손실될 수도 있음에도 차원을 축소하는 이유는 **'차원의 저주'**라고 하는 개념 때문입니다.
- 차원의 저주?

![name1](https://cdn-images-1.medium.com/max/1600/1*GoAgFuRFa8cTWSUB6d2mDA.png)

1. 첫 번째 그림에서 보다시피, 1차원으로 데이터의 42%를 표현할 수 있는 데이터가 있다고 합시다. 같은 데이터에 대해서 차원을 늘릴수록 같은 공간에서 나타낼 수 있는 데이터가 기하급수적으로 감소하는 것을 그림에서 확인할 수 있습니다.

2. 그 말은, 차원이 클수록 분류기와 같은 모델들을 학습시키는 시간도 급격히 증가한다는 의미이며, 모델의 성능또한 급격하게 감소한다는 것을 의미합니다. (noise data의 증가도 모델 성능 향상에 방해를 줌.)

---
&nbsp;
## 2. 차원축소 기법  

&nbsp;  

![](https://i.imgur.com/SxHr0aq.png?1)

**Supervised** : Class Label을 이용해서 특징들을 선택하는 방법  
  -- Filter  : 해당 변수가 추가될시 나타나는 정보 획득 값의 변화만으로 변수를 선택합니다.  
  -- Wrapper : 모델마다 다른 기준에 대해 최적화하는 방향으로 선택합니다. ex)회귀는 : p-value, 의사결정나무 : acc값   
	그러나 변수를 선택하는 것이기 때문에, 선택되지 않는 변수는 지워지는, 즉 정보손실이 크게 발생할 수 있습니다.  
**Unsupervised** : Label 없이 독립변수들만으로 새로운 특징들을 조합, 추출해내는 방법  
-- 종속변수와 관계없이 변수들 자체의 분산을 이용하기에 위의 방법들과는 다르게 항상 같은 결과가 나옵니다.  
-- 여러 변수들의 조합으로 만들어지는 축들이기 때문에 축들의 의미를 설명하기 어렵습니다.

---
&nbsp;
## 3. Unsupervised Methods - Linear Embedding
- 다양한 기법 중, 이번 포스트에서는 주성분분석과 다차원척도법을 다루겠습니다.
	
&nbsp;

### 1. PCA (Principal Component Analysis)
   - 주성분 분석은 변수의 차원을 줄이되, 원 데이터의 분산(변동성)을 가장 잘 보존하는 기저를 찾아내는 기법입니다.  
   output : 주성분 분석의 결과물로는 **새로운 기저와 보존된 분산 값**이 주어지며, 새 좌표를 구하려면 기저벡터를 이용해서 project 시켜야 합니다.
   
![](https://i.imgur.com/Bw9zK4D.png)

##### 1-1. PCA 과정  

   기존의 점들을 새로운 성분의 축으로 project 시켰을 때 '**분산이 최대한 유지**' 되도록 하는 성분을 찾는 것.  
   1. data centering을 진행해서 변수들의 평균을 0으로 setting 합니다. (주성분 분석은 가우시안 분포에 적합하기 때문)

   2. 기존의 data point들에 대한 벡터를 x, 새로운 기저 벡터를 w로 설정합니다.

   3. 기존의 point인 벡터 x를 벡터 w에 project 시킨 벡터를 구하고 project된 point 벡터를 이용해 분산을 표현합니다. project된 점들의 분산을 최대화할 것이므로!  
   $$ w^TX $$  
   $$v=(w^TX)(w^TX)^T=nw^TSw$$  
   4. 그 분산을 최대화하는 식을 정의하고, Lagrangian multiplier를 이용해 문제를 제약식이 없는 문제로 재정의할 수 있습니다.  
   $$max. w^TSw$$  
   $$s.t. w^Tw=1$$  
   5. 재정의된 최대화 문제를 미분하여, 아래처럼 고유값분해가 가능한 형태로 만들어줍니다.  
   $$(S-\lambda I)w =0$$  
   
   6. 고유값 분해를 통해 고유값과 고유벡터를 구해줍니다.  
   
   7. 고유값은 새로운 성분의 분산을 의미하고, 고유벡터는 새로운 성분 벡터를 의미합니다.(구해진 새로운 성분은 기존의 차원 수 만큼 생성되며, 필요에 맞게 성분의 수를 선택할 수 있습니다.)  
   
   $$\lambda _1, \lambda _2, \lambda _3 ...  ,  w_1, w_2, w_3 ...$$
   
##### 1-2. Code 구현
![](https://i.imgur.com/ZEuL0qf.png)

&nbsp;
- uci에서 대표적인 데이터인 Iris 데이터를 이용해서 주성분 분석을 진행해 보겠습니다.

![](https://i.imgur.com/iT1C6zT.png)  
- 위의 그래프는 iris 데이터를 그린 것으로, 각 색깔이 특징을 나타내고 있습니다. 여기서 특징들이 50, 100번째 인덱스를 기준으로 크게 변하는 것을 알 수 있는데 class가 총 3개로 이루어짐을 추측해볼 수 있습니다.
&nbsp;
![](https://i.imgur.com/JzCWpIn.png)  
- 위 그림은 주성분 분석의 첫 단계인 data centering을 거친 결과로, 각 변수의 평균이 0으로 바뀐 것을 알 수 있습니다.

- 이제 이 데이터를 이용해서 공분산 행렬을 만들고, 위의 과정설명에서 언급했듯이 행렬 고유값 분해를 진행합니다. 

- 그러면 새로운 성분들 즉, 새로운 축벡터들을 구할 수 있고 그 벡터들이 내포하는 고유값, 분산 정도를 계산할 수 있습니다.

![](https://i.imgur.com/E6Kx8Ny.png)

[72.77, 23.03, 3.68, 0.51]
[ 72.77% , 95.80% , 99.48% , 100%        ]
- 위의 수치는 고유값의 비율을 높은 순서대로 나열한 것으로, 첫 번째 추출된 성분이 기존 데이터 분산의 72.8%를 설명한다고 해석할 수 있습니다.  
- 그리고 2개의 성분만으로, 기존 변수 4개 중 96%를 설명할 수 있는 것입니다.  
- 아래의 그림에서 성분의 설명력과 누적 설명력의 향상을 확인할 수 있습니다.  
&nbsp;
![](https://i.imgur.com/xqwOGvs.png)
&nbsp;
그럼 여기서 , 몇 개의 성분을 선택하는 것이 바람직할까? 라는 의문이 들 것입니다.
- 주성분 분석에서 성분의 개수를 정할 때 먼저 선택된 성분들이 원 데이터를 얼마나 설명할 수 있는가를 중요하게 생각합니다.

- 일반적으로 추출된 주성분들은 원 데이터의 80% 정도 이상은 설명할 수 있어야 합니다.
&nbsp;
이번에는 고유값의 개념을 살펴보겠습니다.

![](https://i.imgur.com/UhXM7M6.png)

- 위의 그래프는 고유값이 높은 순서대로 그린 것으로 첫 성분의 고유값이 3이라는 것은, pca를 통해 구해진 첫 번째 성분이 원 데이터 특징 4개중 3개정도를 설명할 수 있다고 해석할 수 있습니다.

- 일반적으로 고유값은 1이상인 값들을 고르게 되는데, 1이하의 값을 가진다는 것은 한 개를 추가해도 한 개만큼 설명하지 못한다는 의미이기 때문입니다.


그 이후, 위에서 말했듯이 새로운 좌표를 구하려면 구해진 성분을 이용해서 직접 plot을 해야 합니다.  

![](https://i.imgur.com/yHqCsrW.png)  
![](https://i.imgur.com/g8ckpxj.png)  

이렇게 간단하지만 복잡한 코드를 좀 더 간단히 만들어 놓은 scikit-learn 패키지가 있습니다.  
![](https://i.imgur.com/kRF6UWS.png)
&nbsp;
##### 1-3. PCA와 LDA의 차이
![](https://i.imgur.com/TV2YoiJ.png)  
- 위 그림은 주성분분석으로 구한 2개의 성분으로 두 class를 매핑한 것입니다.  

- 그리고 아래 그림은 선형판별분석(LDA)으로 구한 두 축으로 매핑한 것입니다.

![](https://i.imgur.com/APhHaDe.png)  
- 두 그림을 비교해보면, LDA를 통한 결과가 두 class를 좀 더 잘 분류하고 있음을 알 수 있습니다.  

- 즉, LDA는 분류가 잘 되도록 데이터들의 새로운 축을 찾고, PCA는 단순히 데이터의 분산을 보존하는 축을 찾는 것임을 알 수 있는 비교과정이었습니다.
&nbsp;
##### 1-4. PCA 적용 전후 학습시간 및 결과 비교
아래의 데이터는 별, 은하, 퀘이사인지에 대한 class 변수가 포함된 데이터입니다.  
변수의 종류는 16개, record는 10,000개로 K-means clustering에 대한 학습시간과 결과를 비교해 보았습니다.  
![](https://i.imgur.com/erD6u7E.png)  
- 원 데이터와, 주성분 2개로 표현한 데이터를 이용해서 학습을 진행한 결과,  
full data : 0.102s/76.37%  
pca data : 0.025s/76.37%  
- pca를 적용하기 전과 후 모두 class를 분류하는 성공률은 비슷했지만 pca를 거치면 학습시간이 4배가량 빨라짐을 알 수 있었습니다.  

이렇게 간단한 데이터에서도 보이는 만큼, 데이터의 스케일이 매우 크고 변수의 수가 매우 많은 경우에는 저차원 mapping이 어떠한 모델을 학습하는데 시간적인 비용을 줄여줄 수 있을 것으로 기대할 수 있습니다.
&nbsp;
##### 1-5. 비선형 데이터의 경우
숫자 모양에 대한 데이터 digits을 이용해 선형 매핑과 비선형 매핑을 각각 진행해보았습니다.  
![](https://i.imgur.com/oJevqqK.png)  
- 위의 그림에서 보면 확실히 pca를 이용한 경우가 경계가 좀 더 모호한 것을 알 수 있습니다.  
- 또한 class별로 군집간의 거리정보를 봐도, 클래스마다의 거리를 최대한으로 보존하는 isomap 기법이 결과가 더 좋음을 알 수 있었습니다.  

이와 같이 선형으로 성분을 뽑아내는 PCA 기법으로는 적절하지 않은 데이터들이 현실에서는 매우 많은데, 이는 다음 챕터에서 다룰 예정입니다.
&nbsp;
##### 1-6. PCA의 특징 및 요약
- LDA는 분류를 목적으로 새로운 축을 생성하는 반면, PCA는 단순히 **분산을 유지**하는 방향으로 학습한다.  

- 도수분포표가 좌우대칭인, **가우시안 분포**에 적합하다. - centering 과정이 필요  

- 데이터의 손실은 적지만 원래의 변수의 의미가 복합적으로 반영되므로 **축에 대한 해석이 주관적**일 수 있다.  

- 4번에서도 언급하지만, 회귀분석이나 의사결정나무와 같이 변수의 의미가 중요시되는 모델에는 적합하지 않다.  

- 변수자체의 해석보다는, 데이터의 차원이 너무 커서 모델에 **학습하는 시간 비용을 줄이기 위해** 이용.(영상인식에도 활용)  

- 성분의 개수를 선택하는 것은 자율이나 일반적으로 **설명 비중이 80%, 고유값이 1이상**인 선에서 선택한다.

&nbsp;

### 2. MDS (Multidimensional Scaling)
- 변수의 차원을 줄이면서 원 데이터의 정보(객체 간의 거리 즉, 객체 간 관계)를 가장 잘 보존하는 기저를 찾는 기법입니다.
- PCA와 다르게 객체간의 거리(유사도)행렬만으로 적용 가능.
output : 객체들의 **새로운 좌표**

#####2-1. MDS 과정

   1. 거리(유사도, 비유사도)행렬을 정의해줍니다. (좌표가 주어진 경우, 유클리디언이나 자카드 등 여러 방법을 이용해 데이터들을 거리 행렬로 변환)

   2. 먼저 거리행렬을 다음과 같이 나타낼 수 있습니다. 
    $$D = (x_r-x_s)^T *(x_r-x_s)$$
   3. 위의 식을 전개하고, 시그마를 활용하면 식을 거리행렬의 선형결합으로 나타낼 수 있게 됩니다.
    $$B = x_r^T*x_s = -{1\over2} (d_{rs}^2 - {1\over n}\sum_{s=1}^n d_{rs}^2- {1\over n}\sum_{r=1}^n d_{rs}^2 +{1 \over n^2} \sum_{r=1}^n\sum_{s=1}^n d_{rs}^2)$$
    

   4. 그러면 위의 B가 symmetric하고 positive semi-definite한 행렬임을 알 수 있으므로, 고유값 분해를 진행합니다.
 
 $$B=XX^T=VAV^T , A=diag(\lambda_1, \lambda_2 ... \lambda_n),V=[v_1,v_2...v_n]$$
   
   5. 구해진 고유값 중 non-zero인 수를 모두 선택하면 원 데이터 행렬 X를 구할 수 있습니다. 
   (2차원으로 매핑하고 싶으면 2개의 성분만 선택하여, 2차원 상의 좌표 행렬을 구할 수 있음)

##### 2-2. Code 구현
![](https://i.imgur.com/uT6Qtk5.png)  
![](https://i.imgur.com/8JwTQ2c.png)  
이제 구해진 거리 행렬을 이용해서 B행렬을 정의한 후 고유값 분해를 통해 구한 최종 좌표를 plot 해 보았습니다.

![](https://i.imgur.com/cvaKw6v.png)  
![](https://i.imgur.com/lvCb8lb.png)  
- 거리행렬을 기반으로, 분해한 고유값 중 2개를 선택하여 plot해보니, 분산만을 유지한 pca와 다르게 주황색과 초록색이 조금 더 잘 분리된 것을 볼 수 있었습니다.
- 물론 분류를 목적으로 한 것은 아니지만 원 데이터의 성질상, 다른 class 간의 거리가 유의미했기 때문에 거리 정보를 보존한 MDS 기법에서 그것이 표현되었다고 생각해볼 수 있었습니다.

##### 2-3. MDS를 통한 포지셔닝 맵 시각화 적용(Scikit-learn 패키지 이용)
- 아래의 데이터는 15종류의 자동차에 대해 14가지 특징이 수치화되어있는 데이터입니다.
![](https://i.imgur.com/n51kZRf.png)

- 이 행렬을 이용해 거리 행렬을 정의하고, 축을 2개로 다차원척도법을 적용해서 새로운 좌표를 찍어보았습니다.
![](https://i.imgur.com/qJ86mr4.png)

- 위에 그림에서, 13번째 모델과 14번째 모델이 유사한 제품이라고 해석할 수 있는데, 실제로 1200 VW와 1300 VW로 후속 모델임을 알 수 있었습니다. 즉, 거리가 가까운 객체일수록 비슷한 성향의 제품이라고 해석할 수 있다고 생각합니다.
- 그리고 위의 그래프에 각 변수들의 벡터를 그려보면, 제품마다 어떤 특성에 가까운지 파악도 가능합니다.

##### 2-4. MDS 특징 및 요약

- 원래 차원에서의 거리순서를 고려하면서 저차원으로의 **거리정보가 유지**되도록 하는 차원을 구하는 것.  

- 시각적으로 객체간의 거리를 (유사한 정도를) **쉽게 해석하기 위해**서 저차원으로 mapping 하는 것.  

- 이 또한 변수 자체의 해석보다는, 타겟으로 설정한 변수들이 어떤 관계를 가지는지, 객체들간의 관계를 보기쉽게 **포지셔닝**하는데 이용.

### 3. PCA vs MDS 간단 요약
![](https://i.imgur.com/W5rqJra.png)

## 다음 포스팅에는..

많은 분야에서 활용되고 연구되고 있는 Novelty Detection 에 대한 개념을 살펴보겠습니다.
